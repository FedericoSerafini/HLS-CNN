"""CNN weights generator

This program uses a CNN model built with Tensorflow/Keras to compute
the weights that will be used by the C/HLS network.

Produces a set of header files as output: definitions.h, weights.h.

images size: 28x28

"""

from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import ZeroPadding2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import SGD
from sklearn.model_selection import KFold
import numpy as np
from numpy import mean, size
from numpy import std
from matplotlib import pyplot as plt
from sklearn.model_selection import KFold


conv_1_kernel_size = (3,3)
conv_1_filter_num = 32
pool_1_size = (2,2)




def load_dataset():
	# load dataset
	(trainX, trainY), (testX, testY) = mnist.load_data()
	# reshape dataset to have a single channel
	trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
	testX = testX.reshape((testX.shape[0], 28, 28, 1))
	# one hot encode target values
	trainY = to_categorical(trainY)
	testY = to_categorical(testY)
	return trainX, trainY, testX, testY

def prep_pixels(train, test):
	# convert from integers to floats
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	# normalize to range 0-1
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	# return normalized images
	return train_norm, test_norm

def define_model():
	# define model
	model = Sequential()
	model.add(ZeroPadding2D(padding=1, input_shape=(28, 28, 1)))
	model.add(Conv2D(conv_1_filter_num, conv_1_kernel_size, activation='relu', padding='valid', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
	#model.add(BatchNormalization())
	model.add(MaxPooling2D(pool_1_size))
	model.add(Flatten())
	model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
	#model.add(BatchNormalization())
	model.add(Dense(10, activation='softmax'))
	# compile model
	opt = SGD(learning_rate=0.01, momentum=0.9)
	model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
	return model


# evaluate a model using k-fold cross-validation
def evaluate_model(dataX, dataY, n_folds=5):
	scores, histories = list(), list()
	# prepare cross validation
	kfold = KFold(n_folds, shuffle=True, random_state=1)
	# enumerate splits
	for train_ix, test_ix in kfold.split(dataX):
		# define model
		model = define_model()
		# select rows for train and test
		trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]
		# fit model
		history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)
		# evaluate model
		_, acc = model.evaluate(testX, testY, verbose=0)
		print('> %.3f' % (acc * 100.0))
		# stores scores
		scores.append(acc)
		histories.append(history)
	return model, scores, histories

# plot diagnostic learning curves
def summarize_diagnostics(histories):
	for i in range(len(histories)):
		# plot loss
		plt.subplot(2, 1, 1)
		plt.title('Cross Entropy Loss')
		plt.plot(histories[i].history['loss'], color='blue', label='train')
		plt.plot(histories[i].history['val_loss'], color='orange', label='test')
		# plot accuracy
		plt.subplot(2, 1, 2)
		plt.title('Classification Accuracy')
		plt.plot(histories[i].history['accuracy'], color='blue', label='train')
		plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')
	plt.show()

# summarize model performance
def summarize_performance(scores):
	# print summary
	print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))
	# box and whisker plots of results
	plt.boxplot(scores)
	plt.show()


def save_param_on_file(model):
	# definitions.h
	with open('../C/definitions.h', 'w') as f:
		print('/*\n * This file is auto-generated by gen-weights.py\n */\n',file=f)
		print('#pragma once', file=f)
		print('\n#include <stdint.h>\n\n#define DIGITS 10\n\n#define IMG_ROWS 28\n#define IMG_COLS 28\n',file=f)
		# padding
		print('// Padding.',file=f)
		print( ''
			+ '#define	PAD_ROWS (KRN_ROWS - 1)\n'
			+ '#define	PAD_COLS (KRN_COLS - 1)\n'
			+ '#define PAD_IMG_ROWS (IMG_ROWS + PAD_ROWS)\n'
			+ '#define PAD_IMG_COLS (IMG_COLS + PAD_COLS)\n'
			, file=f)
		# conv_1 constant
		print('// Convolutional layer.',file=f)
		print('#define KRN_ROWS\t',end='',file=f)
		print(conv_1_kernel_size[0], file=f)
		print('#define KRN_COLS\t',end='',file=f)
		print(conv_1_kernel_size[1], file=f)
		print('#define FILTERS\t',end='',file=f)
		print(conv_1_filter_num, file=f, end='\n\n')
		# pool
		print('// Pool layer.\n'
			+ '#define POOL_ROWS\t' + str(pool_1_size[0]) + '\n'
			+ '#define POOL_COLS\t' + str(pool_1_size[1]) + '\n'
			+ '#define POOL_IMG_ROWS (IMG_ROWS / POOL_ROWS)\n'
			+ '#define POOL_IMG_COLS (IMG_COLS / POOL_COLS)\n'
			, file=f)
		# flatten
		print('// Fatten layer.\n'
			+ '#define FLAT_SIZE (FILTERS * POOL_IMG_ROWS * POOL_IMG_COLS)\n'
			, file=f)
		# dense
		print('// Dense layers.\n'
			+ '#define DENSE1_SIZE 100\n'
			+ '#define DENSE2_SIZE 10'
			, file=f
		)

	# conv_weights.h
	with open('../C/conv_weights.h', 'w') as f:
		print('/*\n * This file is auto-generated by gen-weights.py\n */\n',file=f)
		print('#pragma once\n\n#include "definitions.h"\n\n',file=f)
		# save conv_1 weights and bias
		biases, weights = get_params(model.layers[1])
		# conv weights
		print('const float conv_weights [FILTERS][KRN_ROWS][KRN_COLS]\n\t= {',file=f)
		for filter in range(weights.shape[0]):
			print('\t\t\t{', file=f)

			for row in range(weights.shape[1]):
				print('\t\t\t\t{', file=f, end='')

				for col in range(weights.shape[2]):
					print(weights[filter][row][col], file=f, end='')
					if (col != weights.shape[2]-1):
						print(', ', file=f, end='')
				print('}', file=f, end='')

				if(row != weights.shape[1] -1):
					print(',', file=f, end='')
				print(file=f)

			print('\t\t\t}', file=f, end='')
			if(filter != weights.shape[0] -1):
				print(',', file=f, end='')
			print(file=f)
		print('\t\t};\n\n',file=f)

		# conv biases
		print('const float conv_biases [FILTERS] = { ', file=f, end='')
		for i in range(biases.shape[0]):
			print(biases[i], file=f, end='')
			if (i != biases.shape[0]-1):
				print(', ', file=f, end='')
		print('};',file=f)

	# dense_weights.h
	with open('../C/dense_weights.h', 'w') as f:
		dense_layers = []
		for l in model.layers:
			if(l.name.find('dense')>-1):
				dense_layers.append(l)
		print('/*\n * This file is auto-generated by gen-weights.py\n */\n',file=f)
		# dense 1
		print('#pragma once\n\n'
			+ '#include "definitions.h"\n\n'
			+ '// dense 1.'
			, file=f)
		arr = np.transpose(dense_layers[0].weights[0])
		print('float dense1_weights[FLAT_SIZE * DENSE1_SIZE] = {', file=f)
		it = np.nditer(arr, flags=['c_index'])
		for x in it:
			print(str(x) + (',' if it.index != size(arr)-1 else ''), file=f, end=' ')
		print(' };\n', file=f)
		print('float dense1_biases[DENSE1_SIZE] = {',file=f)
		arr = np.array(dense_layers[0].weights[1])
		for x in range(arr.size):
			print(str(arr[x])
				+ (', ' if x != arr.size-1 else '')
				,end=' ', file=f)
		print(' };\n\n', file=f)

		# dense 2
		print('// dense 2.', file=f)
		arr = np.transpose(dense_layers[1].weights[0])
		print('float dense2_weights[FLAT_SIZE * DENSE2_SIZE] = {', file=f)
		it = np.nditer(arr, flags=['c_index'])
		for x in it:
			print(str(x) + (',' if it.index != size(arr)-1 else ''), file=f, end=' ')
		print(' };\n', file=f)
		print('float dense2_biases[DENSE2_SIZE] = {',file=f)
		arr = np.array(dense_layers[1].weights[1])
		for x in range(arr.size):
			print(str(arr[x])
				+ (', ' if x != arr.size-1 else '')
				,end=' ', file=f)
		print(' };\n\n', file=f)


# TO-DO: check if indexing is correct.
def get_params(layer):

	# biases
	b = np.array(layer.weights[1])
	# weights
	w = np.array(layer.weights[0])

	new_w = np.empty(shape=(w.shape[3],w.shape[0],w.shape[1]))

	for r in range(w.shape[0]):
		for c in range(w.shape[1]):
			for f in range(w.shape[3]):
				new_w[f][r][c] = w[r][c][0][f]

	#print("w for each filter")
	#for f in range(new_w.shape[0]):
	#		print(new_w[f])

	#print('b: ', b)

	return b, new_w

def main() -> None:
	print('hello')
    # load dataset
	trainX, trainY, testX, testY = load_dataset()
	print('trainX.shape = ', trainX.shape)
	print('trainY.shape = ', trainY.shape)
	print('testX.shape = ', testX.shape)
	print('testY.shape = ', testY.shape)
	# normalize input images
	trainX, testX = prep_pixels(trainX, testX)


	model = define_model() # temporary
	# evaluate model
	#model, scores, histories = evaluate_model(trainX, trainY)
	# learning curves
	#summarize_diagnostics(histories)
	# summarize estimated performance
	#summarize_performance(scores)



	#print("weights:")
	#layers_number = 1
	#weights_m=model.get_weights()
	#for i in range(layers_number*2):
	#	print(weights_m[i].shape)
	#	save_conv(weights_m[i])


	# save on file
	#layer_with_weights = {1: "conv_layer1"}

	#
	#for idx, layer in enumerate(model.layers):
	#	if(idx in layer_with_weights):
	#		save_conv_layer(layer, idx)




    # train
#    scores, histories = evaluate_model(trainX,trainY)


	save_param_on_file(model)





if __name__ == '__main__':
    main()
	#model = define_model()
	#print(model.summary())
	##save_param_on_file(model)
	#for layer in model.layers:
	#	print(layer)
	#	if np.array(layer.weights).size != 0:
	#		# biases
	#		b = np.array(layer.weights[1])
	#		# weights
	#		w = np.array(layer.weights[0])
	#		wt = np.array(layer.weights[0]).transpose()
#
	#		if(layer.name.find('dense') >- 1):
	#			for i in range(w.shape[0]):
	#				# for each flatten
	#				for j in range(w.shape[1]):
	#					# for each dense
	#					assert(w[i][j] == wt[j][i])
#
	#		print("b: ", b.shape)
	#		print("w: ", w.shape)

